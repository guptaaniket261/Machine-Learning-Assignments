{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A4_ML_NonComp (1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jkNUH8-GIDq"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import time\n",
        "from skimage import io, transform\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt # for plotting\n",
        "import numpy as np\n",
        "import torchvision.models as models\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0G6ter7GZd-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a36f9ab-6e69-4842-e524-37fc1aab0eb9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)\n",
        "# !unzip \"/content/drive/MyDrive/COL774/test_data.zip\" -d \"/content/drive/MyDrive/COL774\"\n",
        "# !unzip \"/content/drive/MyDrive/COL774/train_data.zip\" -d \"/content/drive/MyDrive/COL774\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GE-5HMnGbY9"
      },
      "source": [
        "class Rescale(object):\n",
        "    \"\"\"Rescale the image in a sample to a given size.\n",
        "\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If tuple, output is\n",
        "            matched to output_size. If int, smaller of image edges is matched\n",
        "            to output_size keeping aspect ratio the same.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, image):\n",
        "        h, w = image.shape[:2]\n",
        "        if isinstance(self.output_size, int):\n",
        "            if h > w:\n",
        "                new_h, new_w = self.output_size * h / w, self.output_size\n",
        "            else:\n",
        "                new_h, new_w = self.output_size, self.output_size * w / h\n",
        "        else:\n",
        "            new_h, new_w = self.output_size\n",
        "        new_h, new_w = int(new_h), int(new_w)\n",
        "        img = transform.resize(image, (new_h, new_w))\n",
        "        return img\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, image):\n",
        "        image = image.transpose((2, 0, 1))\n",
        "        return image\n",
        "\n",
        "\n",
        "IMAGE_RESIZE = (224, 224)\n",
        "img_transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.485, 0.456, 0.406), \n",
        "                             (0.229, 0.224, 0.225)) ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skYZJly8GdzF"
      },
      "source": [
        "class CaptionsPreprocessing:\n",
        "    \"\"\"Preprocess the captions, generate vocabulary and convert words to tensor tokens\n",
        "\n",
        "    Args:\n",
        "        captions_file_path (string): captions tsv file path\n",
        "    \"\"\"\n",
        "    def __init__(self, captions_file_path):\n",
        "        self.captions_file_path = captions_file_path\n",
        "\n",
        "        self.raw_captions_dict = self.read_raw_captions()\n",
        "\n",
        "        self.captions_dict = self.process_captions()\n",
        "\n",
        "        self.vocab = self.generate_vocabulary()\n",
        "        self.word_to_ix = {word: i for i, word in enumerate(self.vocab)}\n",
        "        self.ix_to_word = {i : word for i, word in enumerate(self.vocab)}\n",
        "\n",
        "    def read_raw_captions(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            Dictionary with raw captions list keyed by image ids (integers)\n",
        "        \"\"\"\n",
        "\n",
        "        captions_dict = {}\n",
        "        count = 0\n",
        "        with open(self.captions_file_path, 'r', encoding='utf-8') as f:\n",
        "            for img_caption_line in f.readlines():\n",
        "                img_captions = img_caption_line.strip().split('\\t')\n",
        "                img_captions[0] = 'drive/MyDrive/COL774/' + img_captions[0]\n",
        "                captions_dict[img_captions[0]] = img_captions[1]\n",
        "\n",
        "        return captions_dict\n",
        "\n",
        "    def process_captions(self):\n",
        "        \"\"\"\n",
        "        Use this function to generate dictionary and other preprocessing on captions\n",
        "        \"\"\"\n",
        "        raw_captions_dict = self.raw_captions_dict\n",
        "\n",
        "        # Do the preprocessing here\n",
        "        for caption in raw_captions_dict:\n",
        "          raw_captions_dict[caption] = '<start> ' + raw_captions_dict[caption] + ' <end>'\n",
        "\n",
        "        captions_dict = raw_captions_dict\n",
        "        return captions_dict\n",
        "\n",
        "    def generate_vocabulary(self):\n",
        "        \"\"\"\n",
        "        Use this function to generate dictionary and other preprocessing on captions\n",
        "        \"\"\"\n",
        "        captions_dict = self.captions_dict\n",
        "\n",
        "        # Generate the vocabulary\n",
        "        vocab = set()\n",
        "        for i in captions_dict:\n",
        "          tokens = captions_dict[i].split()\n",
        "          for token in tokens:\n",
        "            vocab.add(token)\n",
        "\n",
        "        return vocab\n",
        "\n",
        "\n",
        "    def captions_transform(self, img_caption_list):\n",
        "        \"\"\"\n",
        "        Use this function to generate tensor tokens for the text captions\n",
        "        Args:\n",
        "            img_caption_list: List of captions for a particular image\n",
        "        \"\"\"\n",
        "        vocab = self.vocab\n",
        "        img_caption_token = torch.tensor([self.word_to_ix[w] for w in img_caption_list.split()]).to(torch.int64)\n",
        "        return img_caption_token\n",
        "      \n",
        "CAPTIONS_FILE_PATH = '/content/drive/MyDrive/COL774/Train_text.tsv'\n",
        "captions_preprocessing_obj = CaptionsPreprocessing(CAPTIONS_FILE_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXvNb_NzGfqJ"
      },
      "source": [
        "class ImageCaptionsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, img_dir, captions_dict, img_transform=None, captions_transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_dir (string): Directory with all the images.\n",
        "            captions_dict: Dictionary with captions list keyed by image paths (strings)\n",
        "            img_transform (callable, optional): Optional transform to be applied\n",
        "                on the image sample.\n",
        "            captions_transform: (callable, optional): Optional transform to be applied\n",
        "                on the caption sample (list).\n",
        "        \"\"\"\n",
        "        self.img_dir = img_dir\n",
        "        self.captions_dict = captions_dict\n",
        "        self.img_transform = img_transform\n",
        "        self.captions_transform = captions_transform\n",
        "        self.image_ids = list(captions_dict.keys())\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_ids[idx]\n",
        "        image = io.imread(img_name)\n",
        "        captions = self.captions_dict[img_name]\n",
        "        image = Image.fromarray(np.uint8(image))\n",
        "        # print(img_name)\n",
        "        if self.img_transform:\n",
        "            trnsform = transforms.Resize(IMAGE_RESIZE)\n",
        "            image = trnsform(image)\n",
        "            image = self.img_transform(image)\n",
        "\n",
        "        if self.captions_transform:\n",
        "            captions = self.captions_transform(captions)\n",
        "      \n",
        "        return image, captions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZYenp3LGgX3"
      },
      "source": [
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size, train_CNN=False):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.train_CNN = train_CNN\n",
        "        VGG16 = [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, \"M\", 512, 512, 512, \"M\", 512, 512, 512, \"M\" ]\n",
        "        layers = []\n",
        "        in_ch = 3\n",
        "\n",
        "        for x in VGG16:\n",
        "          if x == \"M\":\n",
        "            layer = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "            layers.append(layer)\n",
        "          else:\n",
        "            out_ch = x\n",
        "            layer = [nn.Conv2d(in_ch, out_ch, kernel_size = (3, 3), padding = (1, 1)), nn.ReLU(inplace = True)]\n",
        "            layers += layer\n",
        "            in_ch = x\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "        self.full_connected = nn.Sequential(                  \n",
        "           nn.Linear(in_features=512*7*7, out_features=4096),\n",
        "           nn.ReLU(inplace=True),\n",
        "           nn.Dropout(p=0.5, inplace=False),\n",
        "           nn.Linear(in_features=4096, out_features=4096),\n",
        "           nn.ReLU(inplace=True),\n",
        "           nn.Linear(in_features=4096, out_features=self.embed_size),\n",
        "           nn.ReLU(inplace=True),\n",
        "          )\n",
        "\n",
        "    def forward(self, images):\n",
        "        features = self.model(images)\n",
        "        features = features.view(features.shape[0],-1)\n",
        "        features = self.full_connected(features)\n",
        "        return features\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        embeddings = self.dropout(self.embed(captions))\n",
        "        embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)\n",
        "        hiddens, _ = self.lstm(embeddings)\n",
        "        outputs = self.linear(hiddens)\n",
        "        return outputs\n",
        "\n",
        "class ImageCaptionsNet(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "        super(ImageCaptionsNet, self).__init__()\n",
        "        self.encoderCNN = EncoderCNN(embed_size)\n",
        "        self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        features = self.encoderCNN(images)\n",
        "        outputs = self.decoderRNN(features, captions)\n",
        "        return outputs\n",
        "\n",
        "    def caption_image(self, image, vocabulary, max_length=50):\n",
        "        result_caption = []\n",
        "        with torch.no_grad():\n",
        "            x = self.encoderCNN(image).unsqueeze(0)\n",
        "            states = None\n",
        "            tok_ind, ind_tok = vocabulary\n",
        "            for i in range(max_length):\n",
        "                hiddens, states = self.decoderRNN.lstm(x, states)\n",
        "                output = self.decoderRNN.linear(hiddens.squeeze(0))\n",
        "                predicted = output.argmax(1)\n",
        "                result_caption.append(predicted.item())\n",
        "                x = self.decoderRNN.embed(predicted).unsqueeze(0)\n",
        "        return [ind_tok[idx] for idx in result_caption]\n",
        "\n",
        "    def caption_image_bm(self, image, vocabulary, max_length=50, beam_width = 3):\n",
        "        result_caption = []\n",
        "        temp_seq = []\n",
        "        with torch.no_grad():\n",
        "            x = self.encoderCNN(image).unsqueeze(0)\n",
        "            states = None\n",
        "            tok_ind, ind_tok = vocabulary\n",
        "            M = nn.Softmax(dim=1)\n",
        "            \n",
        "            hiddens, states = self.decoderRNN.lstm(x, states)\n",
        "            output = self.decoderRNN.linear(hiddens.squeeze(0))\n",
        "            prob, pred_ids = output.topk(beam_width)\n",
        "            output = M(output)\n",
        "            for i in range(pred_ids.shape[1]):\n",
        "              new_token = pred_ids[0, i].item()\n",
        "              new_prob = output[0, pred_ids[0, i].item()].item()\n",
        "              new_state = states\n",
        "              new_x = self.decoderRNN.embed(pred_ids[0, i].unsqueeze(0)).unsqueeze(0)\n",
        "              temp_seq.append([[new_token], new_prob, new_state, new_x])\n",
        "\n",
        "            for _ in range(max_length):\n",
        "              for j in range(len(temp_seq)):\n",
        "                hiddens, states = self.decoderRNN.lstm(temp_seq[j][3], temp_seq[j][2])\n",
        "                output = self.decoderRNN.linear(hiddens.squeeze(0))\n",
        "                prob, pred_ids = output.topk(beam_width)\n",
        "                output = M(output)\n",
        "                for i in range(pred_ids.shape[1]):\n",
        "                  new_token = pred_ids[0, i].item()\n",
        "                  new_prob = output[0, pred_ids[0, i].item()].item()\n",
        "                  new_state = states\n",
        "                  new_x = self.decoderRNN.embed(pred_ids[0, i].unsqueeze(0)).unsqueeze(0)\n",
        "    \n",
        "                  t = temp_seq[j][0]\n",
        "                  t.append(new_token)\n",
        "                  temp_seq.append([t, temp_seq[j][1] + new_prob, new_state, new_x])\n",
        "              temp_seq = temp_seq[beam_width:]  \n",
        "              temp_seq = sorted(temp_seq, reverse=True, key=lambda l: l[1])\n",
        "              temp_seq = temp_seq[:beam_width]\n",
        "\n",
        "        sol = [ind_tok[idx] for idx in temp_seq[0][0]]\n",
        "        return sol[:max_length]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi2YBObLGkvR"
      },
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "  imgs = [item[0].unsqueeze(0) for item in batch]\n",
        "  imgs = torch.cat(imgs, dim = 0)\n",
        "  targets = [item[1] for item in batch]\n",
        "  targets = pad_sequence(targets, batch_first = False)\n",
        "  return imgs, targets\n",
        "\n",
        "IMAGE_DIR = '/content/drive/MyDrive/COL774/train_data'\n",
        "\n",
        "train_dataset = ImageCaptionsDataset(\n",
        "    IMAGE_DIR, captions_preprocessing_obj.captions_dict, img_transform=img_transform,\n",
        "    captions_transform=captions_preprocessing_obj.captions_transform\n",
        ")\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_CNN = False\n",
        "\n",
        "embed_size = 256\n",
        "hidden_size = 256\n",
        "vocab_dict = captions_preprocessing_obj.vocab\n",
        "vocab_size = len(vocab_dict)+ 1\n",
        "num_layers = 1\n",
        "learning_rate = 0.1\n",
        "num_epochs = 15\n",
        "step = 0\n",
        "model = ImageCaptionsNet(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "model.train()\n",
        "\n",
        "directory = '/content/drive/My Drive/COL774/'\n",
        "path = \"model_nonComp_new.pth\"\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=8, collate_fn=collate_fn)\n",
        "for epoch in range(num_epochs):\n",
        "    torch.save(model.state_dict(), directory + path)\n",
        "    for batch_idx, (imgs, captions) in enumerate(train_loader):\n",
        "        imgs = imgs.to(device)\n",
        "        captions = captions.to(device)\n",
        "\n",
        "        outputs = model(imgs, captions[:-1])\n",
        "        loss = criterion(\n",
        "            outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1)\n",
        "        )\n",
        "\n",
        "        step += 1\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward(loss)\n",
        "        optimizer.step()\n",
        "        print(batch_idx, loss.item())\n",
        "  \n",
        "\n",
        "torch.save(model.state_dict(), directory + path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_X9-5cvGowl"
      },
      "source": [
        "img_ids = [i for i in range(1, 5001)]\n",
        "directory = '/content/drive/My Drive/COL774/'\n",
        "path = \"model_nonComp.pth\"\n",
        "class ImageTestDataset(Dataset):\n",
        "    def __init__(self, img_dir, img_ids, img_transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.img_transform = img_transform\n",
        "        self.image_ids = img_ids\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = 'drive/MyDrive/COL774/test_data/test{}.jpg'.format(idx+1) \n",
        "        print(img_name)\n",
        "        image = io.imread(img_name)\n",
        "        image = Image.fromarray(np.uint8(image))\n",
        "        if self.img_transform:\n",
        "            trnsform = transforms.Resize(IMAGE_RESIZE)\n",
        "            image = trnsform(image)\n",
        "            image = self.img_transform(image)\n",
        "        return image\n",
        "\n",
        "cap_model = ImageCaptionsNet(embed_size, hidden_size, vocab_size, num_layers)\n",
        "cap_model.load_state_dict(torch.load(directory + path))\n",
        "cap_model.to(device)\n",
        "\n",
        "Test_dir = '/content/drive/MyDrive/COL774/test_data'\n",
        "\n",
        "img_transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.485, 0.456, 0.406), \n",
        "                             (0.229, 0.224, 0.225)) ])\n",
        "\n",
        "private_test_dataset = ImageTestDataset(\n",
        "    Test_dir, img_ids, img_transform=img_transform,\n",
        "    )\n",
        "\n",
        "private_test_loader = DataLoader(private_test_dataset, batch_size=1, shuffle= False, num_workers=0)\n",
        "\n",
        "count = 0\n",
        "\n",
        "def write_file(batchid,prediction,output_file):\n",
        "    start_tok = \"<start>\"\n",
        "    end_tok = \"<end>\"\n",
        "    tokenless = list(filter((start_tok).__ne__, prediction)) \n",
        "    tokenless = list(filter((end_tok).__ne__, tokenless)) \n",
        "    caption = ' '.join(tokenless)\n",
        "    img_id = \"test_data/test{0}.jpg\".format(batchid+1)\n",
        "    line = [img_id,caption]\n",
        "    output_file.write('\\t'.join(line)+'\\n')\n",
        "\n",
        "##### generate file to print output #####\n",
        "!touch \"/content/drive/MyDrive/COL774/my_file_nonComp.csv\"\n",
        "\n",
        "##### open file to write #####\n",
        "outfile = \"/content/drive/MyDrive/COL774/my_file_nonComp.csv\"\n",
        "output_file = open(outfile,'w') \n",
        "\n",
        "vocabulary = (captions_preprocessing_obj.word_to_ix, captions_preprocessing_obj.ix_to_word)\n",
        "for batchid, sample in enumerate(private_test_loader):\n",
        "  sample = sample.to(device)\n",
        "  prediction = cap_model.caption_image_bm(sample, vocabulary, 8)\n",
        "  # write_file(batchid,prediction,output_file)\n",
        "  print(prediction)\n",
        "  count += 1\n",
        "  if count >= 10:\n",
        "    break\n",
        "\n",
        "output_file.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}